{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: LangChain RAG Project\n",
        "## This Chatboat Have All Information But Only About\n",
        "  * Artificial Intelligence  \n",
        "  * Agentic AI\n",
        "  * Means All About AI"
      ],
      "metadata": {
        "id": "ZVLAxaZhCXxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG-based QA System using LangChain, Pinecone, and Google Generative AI\n",
        "\n",
        "This project notebook demonstrates the creation of a **Retrieval-Augmented Generation (RAG)**-based Question Answering (QA) system. It leverages powerful tools and APIs such as **LangChain**, **Pinecone**, and **Google Generative AI (Gemini Model)** to provide contextual answers to user queries based on uploaded documents. The notebook showcases a step-by-step process, from loading and splitting documents to creating embeddings, uploading data to Pinecone, and querying the system for responses.\n",
        "\n",
        "## Key Features\n",
        "1. **Pinecone for Vector Storage**  \n",
        "   The project uses Pinecone as the vector database to store and retrieve document embeddings efficiently.  \n",
        "   \n",
        "2. **LangChain Integration**  \n",
        "   LangChain simplifies document processing by handling text splitting, embedding generation, and connecting various AI components.  \n",
        "   \n",
        "3. **Google Generative AI (Gemini)**  \n",
        "   Utilizes the Gemini 1.5 model for both embedding generation and language model-based reasoning to deliver intelligent and context-aware responses.  \n",
        "\n",
        "4. **RAG Approach**  \n",
        "   Combines retrieval and generation by searching for the most relevant document chunks and using an LLM to formulate coherent answers.  \n",
        "\n",
        "5. **Flexible Querying**  \n",
        "   Users can input any query, and the system retrieves the most relevant information from the indexed documents to provide accurate answers.  \n",
        "\n",
        "## Workflow\n",
        "1. **Setup**  \n",
        "   Install and configure the required libraries such as LangChain, Pinecone Client, Google Generative AI, and TQDM.  \n",
        "\n",
        "2. **Embedding Generation**  \n",
        "   Process documents into chunks and generate embeddings using Google Generative AI's embedding model.  \n",
        "\n",
        "3. **Vector Indexing**  \n",
        "   Store the generated embeddings in a Pinecone index for efficient similarity-based retrieval.  \n",
        "\n",
        "4. **Query Handling**  \n",
        "   User queries are processed through a retrieval-augmented chain that fetches the most relevant documents, formulates a response, and displays the result.\n",
        "\n",
        "5. **Source Document Traceability**  \n",
        "   Returns the source documents used in formulating each response for transparency and validation.\n",
        "\n",
        "## Use Cases\n",
        "- Building document-based question-answering systems.\n",
        "- Enhancing productivity by allowing quick access to relevant information from large datasets.\n",
        "- Supporting tasks in education, research, and knowledge management.\n",
        "\n",
        "## Technologies Used\n",
        "- **LangChain**: Framework for building AI-powered applications.\n",
        "- **Pinecone**: Vector database for storing and retrieving embeddings.\n",
        "- **Google Generative AI (Gemini Model)**: Advanced generative AI for embedding and LLM capabilities.\n",
        "- **Python**: Programming language for integrating APIs and data processing.\n",
        "\n",
        "This project highlights the potential of combining vector search with generative AI to enable sophisticated knowledge retrieval systems.\n"
      ],
      "metadata": {
        "id": "AkoHY-em34m2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u9Fx-pmQCLmh"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-pinecone tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "PINECONE_ENVIRONMENT = 'us-east-1'"
      ],
      "metadata": {
        "id": "Rc6yaOIgGbQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "\n",
        "pc = Pinecone(\n",
        "    api_key=PINECONE_API_KEY\n",
        ")\n",
        "\n",
        "# Check if the index exists; if not, create it\n",
        "index_name = \"anewindex\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",  # Choose the metric: cosine, euclidean, or dotproduct\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=PINECONE_ENVIRONMENT  # Use your environment's region\n",
        "        )\n",
        "    )\n",
        "\n",
        "# # Connect to the index\n",
        "index = pc.Index(name=index_name)\n",
        "print(f\"Successfully connected to index: {index_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muJ6Mz0DHb35",
        "outputId": "01f41903-433f-4603-ff38-dc529017c5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to index: anewindex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnWIARJrLK75",
        "outputId": "c1900380-62f6-4f0f-ed80-6d751165e584",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY # Assuming PINECONE_API_KEY is already defined\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",  # Specify the desired embedding model\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "dXGcUcqeKcah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoDT7AgaLn_x",
        "outputId": "68f0b737-baed-450a-d482-020142344a50",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"/content/document.txt\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "NH-wCojMLd8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "\n",
        "    # Modify the upsert to use a dictionary for metadata\n",
        "    index.upsert([{\n",
        "        \"id\": doc.metadata[\"source\"],  # Use \"id\" instead of the first tuple element\n",
        "        \"values\": vector,  # Use \"values\" for the vector\n",
        "        \"metadata\": {\n",
        "            \"text\": doc.page_content,  # Add the text as part of metadata\n",
        "            \"source\": doc.metadata[\"source\"]  # Include the source\n",
        "        }\n",
        "    }])\n"
      ],
      "metadata": {
        "id": "D2VCw_OfFbzL",
        "outputId": "8f9d8d59-e6db-4034-dfdd-4b3156981a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:10<00:00,  2.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "# Use from_existing_index to load from an existing index\n",
        "retriever = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key=\"text\")"
      ],
      "metadata": {
        "id": "-09FA0Opzqn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough,RunnableMap\n",
        "\n",
        "message = \"\"\"\n",
        "Answer this question using the provided context only.\n",
        "\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "D45Qz23sfLI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])"
      ],
      "metadata": {
        "id": "CdGvFWNb9och"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY,model=\"gemini-1.5-flash\", temperature=0.7)"
      ],
      "metadata": {
        "id": "-8OUqrpu0Gdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"])[0], # Get the first relevant document\n",
        "    \"question\": RunnablePassthrough()\n",
        "}) | prompt | gemini_model\n"
      ],
      "metadata": {
        "id": "h5hdO9Fr9sPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a vector store using the Pinecone index\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "# Create the retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # Retrieve top 2 most similar documents\n",
        "\n",
        "# Create the QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,  # Pass the retriever here\n",
        "    return_source_documents=True  # Optional: to get the source documents used in the response\n",
        ")"
      ],
      "metadata": {
        "id": "pEMQYqQk1Dgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.similarity_search(\"Future?\", k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZbA7CDsOL0",
        "outputId": "2eb0b0dd-b628-4b69-e7a2-c5654e81a5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='/content/document.txt', metadata={'source': '/content/document.txt'}, page_content='The progress in Generative AI and Agentic AI will lead to smarter, more adaptive systems that can autonomously generate solutions, create content, and interact with the world in more natural, human-like ways.')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Welcome To Hasnain's Coding World\")\n",
        "print('='*40)\n",
        "\n",
        "query = \"What is The FUture Of Agents?\"\n",
        "print('Human Message:',query)\n",
        "response = qa_chain.invoke(query)\n",
        "\n",
        "# Print the answer\n",
        "print(\"Agent Message:\", response['result'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_3z1a9t1YZc",
        "outputId": "9b645ce6-31ef-4d0a-fa67-0401b1c81972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome To Hasnain's Coding World\n",
            "========================================\n",
            "Human Message: What is The FUture Of Agents?\n",
            "Agent Message: Based on the provided text, the future of agents will involve smarter, more adaptive systems capable of autonomously generating solutions, creating content, and interacting with the world in more natural, human-like ways.  This is driven by advancements in Generative AI and Agentic AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Welcome To Hasnain's Coding World\")\n",
        "print('='*40)\n",
        "\n",
        "query = \"What is the Future of AI Agents?\"\n",
        "print('Human Message:',query)\n",
        "response = qa_chain.invoke(query)\n",
        "\n",
        "# Print the answer\n",
        "print(\"Agent Message:\", response['result'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgIiTDdX8waY",
        "outputId": "3d89e9e7-81fa-454c-d44c-ce66c030ad63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome To Hasnain's Coding World\n",
            "========================================\n",
            "Human Message: What is the Future of AI Agents?\n",
            "Agent Message: Based on the provided text, the future of AI agents involves smarter, more adaptive systems capable of autonomously generating solutions, creating content, and interacting with the world in more natural, human-like ways.\n",
            "\n",
            " According To Given Information:\n",
            "- The progress in Generative AI and Agentic AI will lead to smarter, more adaptive systems that can autonomously generate solutions, create content, and interact with the world in more natural, human-li...\n"
          ]
        }
      ]
    }
  ]
}